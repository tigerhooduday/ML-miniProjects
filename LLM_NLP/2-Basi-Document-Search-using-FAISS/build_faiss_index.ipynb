{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c0a6ab9c",
   "metadata": {},
   "source": [
    "# Build the index (script)\n",
    "\n",
    "Save this as build_faiss_index.py. It:\n",
    "- reads PDFs page-by-page\n",
    "- chunks text into overlapping chunks\n",
    "- computes embeddings in batches\n",
    "- normalizes and writes FAISS index\n",
    "- saves metadata & chunks (pickle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "000869ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# build_faiss_index.py\n",
    "import os, glob, pickle, argparse\n",
    "from PyPDF2 import PdfReader\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np\n",
    "import faiss\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7c3f0ef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Config â€” tune these\n",
    "MODEL_NAME = \"all-MiniLM-L6-v2\"\n",
    "CHUNK_SIZE = 1000      # characters per chunk\n",
    "CHUNK_OVERLAP = 200    # overlap between chunks\n",
    "BATCH_SIZE = 64        # embedding batch size\n",
    "OUT_INDEX = \"data/faiss.index\"\n",
    "OUT_META = \"data/meta.pkl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "817222fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text_from_pdf(path):\n",
    "    reader = PdfReader(path)\n",
    "    pages = []\n",
    "    for pnum, page in enumerate(reader.pages):\n",
    "        text = page.extract_text() or \"\"\n",
    "        text = text.replace(\"\\r\", \" \")\n",
    "        pages.append((pnum+1, text))\n",
    "    return pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8ffbdf78",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_text(text, size=CHUNK_SIZE, overlap=CHUNK_OVERLAP):\n",
    "    text = text.strip()\n",
    "    if not text:\n",
    "        return []\n",
    "    chunks = []\n",
    "    start = 0\n",
    "    L = len(text)\n",
    "    while start < L:\n",
    "        end = start + size\n",
    "        chunk = text[start:end].strip()\n",
    "        if chunk:\n",
    "            chunks.append(chunk)\n",
    "        if end >= L:\n",
    "            break\n",
    "        start = max(end - overlap, start + 1)\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e9c0c488",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_index(pdf_folder, out_index=OUT_INDEX, out_meta=OUT_META):\n",
    "    model = SentenceTransformer(MODEL_NAME)\n",
    "    all_chunks = []\n",
    "    metadatas = []  # list of dicts: {source, page, chunk_id}\n",
    "    files = sorted(glob.glob(os.path.join(pdf_folder, \"*.pdf\")))\n",
    "    print(f\"Found {len(files)} pdf files.\")\n",
    "\n",
    "    for path in files:\n",
    "        pages = extract_text_from_pdf(path)\n",
    "        for page_num, text in pages:\n",
    "            chunks = chunk_text(text)\n",
    "            for i, c in enumerate(chunks):\n",
    "                all_chunks.append(c)\n",
    "                metadatas.append({\"source\": os.path.basename(path), \"page\": page_num, \"chunk_id\": i})\n",
    "\n",
    "    if not all_chunks:\n",
    "        raise SystemExit(\"No text extracted from PDFs. Check your PDF files or use OCR for scanned PDFs.\")\n",
    "    \n",
    "    # embeddings (float32)\n",
    "    print(\"Computing embeddings...\")\n",
    "    embeddings = model.encode(all_chunks, batch_size=BATCH_SIZE, show_progress_bar=True, convert_to_numpy=True)\n",
    "    embeddings = embeddings.astype(\"float32\")\n",
    "\n",
    "    # normalize -> use inner product for cosine sim\n",
    "    faiss.normalize_L2(embeddings)\n",
    "\n",
    "    d = embeddings.shape[1]\n",
    "    index = faiss.IndexFlatIP(d)  # exact search, inner product (works as cosine if normalized)\n",
    "    index.add(embeddings)\n",
    "    os.makedirs(os.path.dirname(out_index), exist_ok=True)\n",
    "    faiss.write_index(index, out_index)\n",
    "\n",
    "    # save metadata & chunks\n",
    "    with open(out_meta, \"wb\") as f:\n",
    "        pickle.dump({\"chunks\": all_chunks, \"metadatas\": metadatas}, f)\n",
    "\n",
    "    print(f\"Index saved to {out_index}, metadata saved to {out_meta}. Total chunks: {len(all_chunks)}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "62e03c3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] [--pdf_folder PDF_FOLDER]\n",
      "                             [--out_index OUT_INDEX] [--out_meta OUT_META]\n",
      "ipykernel_launcher.py: error: unrecognized arguments: --f=c:\\Users\\Udayg\\AppData\\Roaming\\jupyter\\runtime\\kernel-v3fd9e10017a5fdb18468f0c27b2b3d1153fa90d29.json\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[31mSystemExit\u001b[39m\u001b[31m:\u001b[39m 2\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    p = argparse.ArgumentParser()\n",
    "    p.add_argument(\"--pdf_folder\", default=\"pdfs\")\n",
    "    p.add_argument(\"--out_index\", default=OUT_INDEX)\n",
    "    p.add_argument(\"--out_meta\", default=OUT_META)\n",
    "    args = p.parse_args()\n",
    "    build_index(args.pdf_folder, args.out_index, args.out_meta)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "virtualenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
